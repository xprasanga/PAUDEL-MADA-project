---
title: "robustness-check"
format: html
editor: visual
---

NOTE: Not all codes may produce an output, you can find those outputs saved in the respective folder location.

```{r}
# Loading required packages
library(purrr)    # For map_df()
library(dplyr)    # For data manipulation
library(timetk)   # For time_series_cv()
library(rsample)  # For analysis() and assessment()
library(here)
library(modelsummary)
library(tidyr)
library(ranger) 
library(knitr)
library(kableExtra)

```

# Importing dataset

Importing train/test dataset.

```{r}
train <- readRDS(here("data", "processed-data", "train-data.rds"))
test <- readRDS(here("data", "processed-data", "test-data.rds"))
```

# OLS regression on dataset

Here, we will be testing the regression model on out test data

```{r}
# primary regression model on test data

model <- lm(Current_health_expenditure_per_capita_current_US ~ 
              GDP_growth_annual_ + 
              Income_share_held_by_lowest_20 + 
              Control_of_Corruption_Estimate + 
              Life_expectancy_at_birth_total_years + 
Unemployment_youth_total__of_total_labor_force_ages_1524_modeled_ILO_estimate + 
              Trade__of_GDP+ Foreign_direct_investment_net_inflows_BoP_current_US+ 
  Access_to_electricity_rural__of_rural_population +
  Age_dependency_ratio__of_workingage_population
  , 
            data = test)

modelsummary(
  model,
  output = here("results", "tables", "testresult.png"),
  title = "This table represents regresssion results from the test data",
  stars = c('*' = 0.1, '**' = 0.05, '***' = 0.01),
  notes = list(
    "Standard errors are shown in parentheses.",
    "Significance levels: * p < 0.1, ** p < 0.05, *** p < 0.01."
  )
)

```

The results from our test dataset is show moderate performance of our model. Check the manuscript for more details on the comparision between train and test results.

# cross validation of OLS model

Here, we will use cross validation to test the robustness of our model. We will have 5 year initial window and 1 year test window.

```{r}


# defining the model  
model_formula <- Current_health_expenditure_per_capita_current_US ~ 
  GDP_growth_annual_ + 
  Income_share_held_by_lowest_20 + 
  Control_of_Corruption_Estimate + 
  Life_expectancy_at_birth_total_years + 
  Unemployment_youth_total__of_total_labor_force_ages_1524_modeled_ILO_estimate + 
  Trade__of_GDP + 
  Foreign_direct_investment_net_inflows_BoP_current_US + 
  Access_to_electricity_rural__of_rural_population +
  Age_dependency_ratio__of_workingage_population

#creating time-series CV splits
cv_splits <- time_series_cv(
  data = train,
  date_var = Year,
  initial = 5,  # 5-year initial window
  assess = 1,   # 1-year test window
  skip = 1,     # Move forward 1 year each split
  slice_limit = 3
)
```

```{r}

set.seed(123)

cv <- function(splits, model_formula) {
  map_df(splits, function(split) {
    train_cv <- analysis(split)
    test_cv <- assessment(split)
    
    #training model
    model <- lm(model_formula, data = train_cv)
    
    # Getting predictions and actuals
    preds <- predict(model, newdata = test_cv)
    actuals <- test_cv$Current_health_expenditure_per_capita_current_US
    
    #converting to numeric 
    preds_num <- suppressWarnings(as.numeric(preds))
    actuals_num <- suppressWarnings(as.numeric(actuals))
    
    #calculating RMSE
    if(length(preds_num) == 0 || length(actuals_num) == 0) {
      rmse_val <- NA_real_
      status <- "Empty predictions/actuals"
    } else if(any(is.na(actuals_num))) {
      valid <- !is.na(actuals_num)
      rmse_val <- sqrt(mean((preds_num[valid] - actuals_num[valid])^2, na.rm = TRUE))
      status <- "NA in actuals"
    } else {
      rmse_val <- sqrt(mean((preds_num - actuals_num)^2, na.rm = TRUE))
      status <- "Success"
    }
    
    data.frame(
      Train_Years = paste(min(train_cv$Year), max(train_cv$Year), sep = "-"),
      Test_Year = max(test_cv$Year),
      RMSE = rmse_val,
      stringsAsFactors = FALSE
    )
  })
}

#saving the results
results <- cv(cv_splits$splits, model_formula)
print(results)

#saving the results
#saving the table
cv_results <- kable(results, format = "html", digits = 3) %>%
kable_styling()
save_kable(cv_results, file = here("results", "tables","OLS-CV-results-full.html"))
```

Now we will see the summary of the CV results

```{r}
#view summary of CV results
cv_summary <- results %>%
  summarise(
    Mean_RMSE = mean(RMSE, na.rm = TRUE),
    SD_RMSE = sd(RMSE, na.rm = TRUE),
  )

print(cv_summary)

#saving the table
cv_summary_kable <- kable(cv_summary, format = "html", digits = 3) %>%
kable_styling()
save_kable(cv_summary_kable, file = here("results", "tables","OLS-CV-results.html"))
```

The mean RMSE is 1151 which is simillar to the RMSE found in the original result (RMSE=1213). Therefore CV estimates show that our result is quite robust with very low Standard Deviation of 74.8

We will also test if there is any other model that performs better than OLS regression. We will use Random Forest model.

# Random Forest

## Random Forest using train dataset

```{r}

target_var <- all.vars(model_formula)[1]
train_processed <- train %>%
  drop_na(all_of(target_var))

#calculate appropriate mtry
n_predictors <- length(all.vars(model_formula)) - 1
mtry_value <- max(1, min(floor(n_predictors / 3), n_predictors))


# Train Random Forest on full training dataset
set.seed(123)
rf_full_model <- ranger(
  formula = model_formula,
  data = train_processed,
  num.trees = 500,
  mtry = mtry_value,
  importance = 'permutation',
  seed = 123
)

# Extract variable importance
importance_df <- data.frame(
  Variable = names(rf_full_model$variable.importance),
  Importance = rf_full_model$variable.importance
) %>% 
  arrange(desc(Importance))

# Create importance plot
library(ggplot2)
importance_plot <- ggplot(importance_df, aes(x = reorder(Variable, Importance), 
                         y = Importance)) +  # Fixed this line - removed extra parenthesis
  geom_col(fill = "steelblue") +  # Simplified this line
  coord_flip() +
  labs(title = "Random Forest Variable Importance",
       x = "Predictor Variables",
       y = "Importance (Permutation)") +
  theme_minimal()

# Save the plot
ggsave(here("results", "figures", "rf_importance_plot.png"), 
       plot = importance_plot,
       width = 8, height = 6, dpi = 300)
```

Here we will calcualte the performance metrics of our RF model.

```{r}
train_predictions <- predict(rf_full_model, data = train_processed)$predictions

# Calculate performance metrics
train_performance <- train_processed %>%
  mutate(Predicted = train_predictions) %>%
  summarise(
    RMSE = sqrt(mean((.data[[target_var]] - Predicted)^2)),
    MAE = mean(abs(.data[[target_var]] - Predicted)),
    R2 = cor(.data[[target_var]], Predicted)^2
  ) %>%
  mutate(across(everything(), ~round(., 3)))  # Round all metrics to 3 decimal places

# Display the performance metrics
print(train_performance)

```

Saving the table

```{r}
library(kableExtra)
 train_table <- train_performance %>%
  pivot_longer(everything(), names_to = "Metric", values_to = "Value") %>%
  mutate(Metric = case_when(
    Metric == "RMSE" ~ "Root Mean Squared Error",
    Metric == "MAE" ~ "Mean Absolute Error",
    Metric == "R2" ~ "R-squared",
    TRUE ~ Metric
  )) %>%
  kable(format = "html", align = c("l", "r")) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))

kableExtra::save_kable(train_table, 
                       file = here("results", "tables", "rf_train_performance.html"))
```

## Random Forest with CV on train dataset

```{r}
rf_cv <- function(splits, model_formula, include_importance = TRUE) {
  map_df(splits, function(split) {
    train_cv <- analysis(split)
    test_cv <- assessment(split)
    
    # setting the target variables
    target_var <- all.vars(model_formula)[1]
    
    # checking if Year column exists
    year_col <- grep("^year$", names(train_cv), ignore.case = TRUE, value = TRUE)
    
    #selecting required variables
    required_vars <- unique(c(all.vars(model_formula), year_col))
    train_cv <- train_cv %>% 
      select(any_of(required_vars)) %>% 
      drop_na(all_of(target_var))
    
    test_cv <- test_cv %>% 
      select(any_of(required_vars)) %>% 
      drop_na(all_of(target_var))
    
    # getting years
    get_years <- function(df) {
      if (length(year_col) > 0) {
        list(
          min_year = min(df[[year_col]], na.rm = TRUE),
          max_year = max(df[[year_col]], na.rm = TRUE)
        )
      } else {
        list(min_year = NA_real_, max_year = NA_real_)
      }
    }
    
    train_years <- get_years(train_cv)
    test_year <- get_years(test_cv)$max_year
    
    #calculate mtry
    n_predictors <- length(setdiff(required_vars, c(target_var, year_col)))
    mtry_value <- max(1, min(floor(n_predictors / 3), n_predictors))
    
    #train model
    set.seed(123)
    rf_model <- ranger(
      formula = model_formula,
      data = train_cv,
      num.trees = 500,
      mtry = mtry_value,
      importance = 'permutation',
      seed = 123
    )
    
    #calculate RMSE
    preds <- predict(rf_model, data = test_cv)$predictions
    actuals <- pull(test_cv, target_var)
    rmse_val <- sqrt(mean((preds - actuals)^2, na.rm = TRUE))
    
    #return results
    tibble(
      Train_Years = if (!is.na(train_years$min_year)) {
        paste(train_years$min_year, train_years$max_year, sep = "-")
      } else {NA_character_},
      Test_Year = test_year,
      RMSE = rmse_val,
      Model = "Random Forest",
      Predictors_Used = n_predictors,
      mtry_Used = mtry_value,
      Year_Column_Found = ifelse(length(year_col) > 0, year_col, "Not found")
    )
  })
}
```

```{r}
rf_results <- rf_cv(cv_splits$splits, model_formula)
print(rf_results)
```

The results of Random Forest are also quite good. They show very low RMSE compared to the OLS Moldel.

Saving the RF results

```{r}
# Select only the desired columns
rf_results_table <- rf_results %>%
  select(Train_Years, Test_Year, RMSE, Predictors_Used, mtry_Used)

# Create and save the HTML table
rf_results_html <- kable(rf_results_table, 
                         format = "html",
                         digits = 2,
                         caption = "") %>%
  kable_styling(bootstrap_options = c("striped", "hover"),
                full_width = FALSE,
                position = "center") %>%
  add_header_above(c("Training Period" = 1, "Test Year" = 1, 
                     "Performance" = 1, "Model Parameters" = 2))

# Save the table
save_kable(rf_results_html, 
           file = here("results", "tables", "rf_cv_results.html"))
```

Now we will calculate the summary of the above results.

```{r}

# Create summary table first
rf_summary_table <- rf_results %>%
  summarise(
    `Mean RMSE` = mean(RMSE, na.rm = TRUE),
    `SD RMSE` = sd(RMSE, na.rm = TRUE),
    `Min RMSE` = min(RMSE, na.rm = TRUE),
    `Max RMSE` = max(RMSE, na.rm = TRUE)
  ) %>%
  mutate(across(everything(), ~round(., 3)))  # Round all values

# Then create and save HTML table
rf_summary_html <- kable(rf_summary_table, 
                         format = "html",
                         caption = "") %>%
  kable_styling(bootstrap_options = c("striped", "hover"),
                full_width = FALSE,
                position = "center") %>%
  row_spec(0, bold = TRUE)

save_kable(rf_summary_html, file = here("results", "tables", "rf_cv_summary.html"))
```

# Random Forest on test dataset

```{r}
library(tidyverse)
library(ranger)

#preparing training data
target_var <- all.vars(model_formula)[1]
train_processed <- train %>%
  drop_na(all_of(target_var))

#calculate appropriate mtry
n_predictors <- length(all.vars(model_formula)) - 1
mtry_value <- max(1, min(floor(n_predictors / 3), n_predictors))

#training Random Forest model
set.seed(123)
rf_model <- ranger(
  formula = model_formula,
  data = train_processed,
  num.trees = 500,
  mtry = mtry_value,
  importance = 'permutation',
  seed = 123
)
```

```{r}
#processing test data
test_processed <- test %>%
  select(any_of(names(train_processed))) %>%  # keeping only columns present in training
  drop_na(all_of(target_var))  # Removing rows with NA in target
```

```{r}
# Generating predictions
test_predictions <- predict(rf_model, data = test_processed)$predictions

#creating results dataframe
test_results <- test_processed %>%
  mutate(Predicted = test_predictions,
         Residual = .data[[target_var]] - Predicted)
```

```{r}
# Calculate metrics manually
performance_metrics <- data.frame(
  RMSE = sqrt(mean(test_results$Residual^2)),
  MAE = mean(abs(test_results$Residual)),
  R2 = cor(test_results[[target_var]], test_results$Predicted)^2
)

print(performance_metrics)
```

```{r}

# Format the performance metrics into a clean table
performance_table <- performance_metrics %>%
  pivot_longer(cols = everything(), 
               names_to = "Metric", 
               values_to = "Value") %>%
  mutate(Metric = case_when(
    Metric == "RMSE" ~ "Root Mean Squared Error",
    Metric == "MAE" ~ "Mean Absolute Error",
    Metric == "R2" ~ "R-squared",
    TRUE ~ Metric
  ),
  Value = round(Value, 3))  # Round to 3 decimal places

# Create and save the HTML table
performance_html <- kable(performance_table, 
                         format = "html",
                         col.names = c("Performance Metric", "Value"),
                         align = c("l", "r"),
                         caption = "") %>%
  kable_styling(bootstrap_options = c("striped", "hover"),
                full_width = FALSE,
                position = "center") %>%
  row_spec(0, bold = TRUE, background = "#f8f9fa") %>%
  column_spec(2, width = "5cm")

# Save the table
save_kable(performance_html, 
           file = here("results", "tables", "rf_test_performance.html"))

```

However, the results from test dataset are not promising for our Random Forest model. So, we will stick with our OLS model.

# Visualizing the models train dataset

```{r}
# For Random Forest model 
rf_train_predictions <- predict(rf_full_model, data = train_processed)$predictions

# For OLS model 
ols_model <- lm(model_formula, data = train_processed)
ols_train_predictions <- predict(ols_model, newdata = train_processed)

#creating combined data frame for plotting
plot_data <- train_processed %>%
  mutate(
    RF_Predicted = rf_train_predictions,
    OLS_Predicted = ols_train_predictions,
    Observed = .data[[target_var]]
  )

#creating the comparison plot
library(ggplot2)
pred_vs_obs_plot <- ggplot(plot_data, aes(x = Observed)) +
  geom_point(aes(y = RF_Predicted, color = "Random Forest"), alpha = 0.6) +
  geom_point(aes(y = OLS_Predicted, color = "OLS Regression"), alpha = 0.6) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black") +
  labs(
    title = "Predicted vs Observed Values (Training Data)",
    x = "Observed Health Expenditure per Capita (US$)",
    y = "Predicted Health Expenditure per Capita (US$)",
    color = "Model Type"
  ) +
  scale_color_manual(values = c("Random Forest" = "#1f77b4", 
                               "OLS Regression" = "#ff7f0e")) +
  theme_minimal() +
  theme(legend.position = "bottom")

#saing the plot
ggsave(here("results", "figures", "pred_vs_obs_train.png"),
       plot = pred_vs_obs_plot,
       width = 8, height = 6, dpi = 300)

# Display  plot
print(pred_vs_obs_plot)
```

# Visualizing the models test dataset

```{r}
#generating predictions for test data as we did for train data

# Random Forest predictions 
rf_test_predictions <- predict(rf_model, data = test_processed)$predictions

# OLS predictions on test data
ols_test_predictions <- predict(model, newdata = test_processed)  #

# combined data frame for plotting
test_plot_data <- test_processed %>%
  mutate(
    RF_Predicted = rf_test_predictions,
    OLS_Predicted = ols_test_predictions,
    Observed = .data[[target_var]]
  )

# comparison plot
test_pred_vs_obs_plot <- ggplot(test_plot_data, aes(x = Observed)) +
  geom_point(aes(y = RF_Predicted, color = "Random Forest"), alpha = 0.6, size = 2) +
  geom_point(aes(y = OLS_Predicted, color = "OLS Regression"), alpha = 0.6, size = 2) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black") +
  labs(
    title = "Predicted vs Observed Values (Test Data)",
    x = "Observed Health Expenditure per Capita (US$)",
    y = "Predicted Health Expenditure per Capita (US$)",
    color = "Model Type"
  ) +
  scale_color_manual(values = c("Random Forest" = "#E69F00",  
                               "OLS Regression" = "#56B4E9")) +  
  theme_minimal() +
  theme(legend.position = "bottom",
        plot.title = element_text(hjust = 0.5)) +  # Center title
  coord_equal()  # Equal axis scaling for proper 45° line

#adding performance metrics to plot
test_performance <- test_plot_data %>%
  summarise(
    RF_RMSE = sqrt(mean((Observed - RF_Predicted)^2)),
    OLS_RMSE = sqrt(mean((Observed - OLS_Predicted)^2))
  )

test_pred_vs_obs_plot <- test_pred_vs_obs_plot +
  annotate("text", x = min(test_plot_data$Observed), 
           y = max(c(test_plot_data$RF_Predicted, test_plot_data$OLS_Predicted)),
           hjust = 0, vjust = 1,
           label = sprintf("RMSE:\nRandom Forest: %.1f\nOLS: %.1f",
                          test_performance$RF_RMSE, 
                          test_performance$OLS_RMSE))

#saving the plot
ggsave(here("results", "figures", "pred_vs_obs_test.png"),
       plot = test_pred_vs_obs_plot,
       width = 8, height = 6, dpi = 300)

#show the plot
print(test_pred_vs_obs_plot)
```
